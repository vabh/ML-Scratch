{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n",
      "0.2.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as dtrans\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f02edab0a70>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "batch_size = 64\n",
    "torch.manual_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform =  dtrans.Compose([dtrans.ToTensor()])\n",
    "# train_data = dset.MNIST(root='../../data/', train=True, transform=transform)\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# transform =  dtrans.Compose([dtrans.ToTensor()])\n",
    "# test_data = dset.MNIST(root='../../data/', train=False, transform=transform)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform =  dtrans.Compose([dtrans.RandomHorizontalFlip(),\n",
    "                             dtrans.RandomCrop(size=(32,32), padding=(4,4)),\n",
    "                             dtrans.ToTensor(),\n",
    "                             dtrans.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "train_data = dset.CIFAR10(root='../../data/', train=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "transform =  dtrans.Compose([dtrans.ToTensor(),\n",
    "                            dtrans.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "test_data = dset.CIFAR10(root='../../data/', train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookAhead(optim.Optimizer):\n",
    "    def __init__(self, params, alpha, k, base_optim, A_args):\n",
    "        self.alpha = alpha\n",
    "        self.A = base_optim\n",
    "        self.A_args = A_args\n",
    "        self.k = k\n",
    "        self.inner_step = 0\n",
    "        \n",
    "        self._sync_params(params)\n",
    "#         self._reset_A(self.theta, A_args)\n",
    "        \n",
    "        defaults = {}\n",
    "        super(LookAhead, self).__init__(self.phi, defaults)\n",
    "        \n",
    "    def _sync_params(self, params):\n",
    "        with torch.no_grad():\n",
    "            self.theta = list(params)\n",
    "            self.phi = []\n",
    "            for p in self.theta:\n",
    "                new_p = torch.tensor(p.data)\n",
    "                self.phi.append(new_p)\n",
    "                \n",
    "#     def _reset_A(self, params, A_args):\n",
    "#         self.A = optim.SGD(params, **A_args)\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        if self.inner_step == self.k:   \n",
    "            self.inner_step = 0\n",
    "    \n",
    "            for pg1, pg2 in zip(self.param_groups, self.A.param_groups):\n",
    "                for p1, p2 in zip(pg1['params'], pg2['params']):\n",
    "                    p1.data = p1.data + self.alpha * (p2.data - p1.data)\n",
    "                    p2.data = p1.data.clone()\n",
    "        else:\n",
    "            self.inner_step += 1\n",
    "            self.A.step()\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        self.A.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28*1, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimiser, train_loader, device):\n",
    "    model.train()\n",
    "    \n",
    "    loss_epoch, error = 0, 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        out = model(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        loss_epoch += loss.item()\n",
    "        error += (out.argmax(1) != labels).sum().item()\n",
    "        \n",
    "    print('[Train] Loss: %.4f, Err: %d,' %(loss_epoch/i, error))\n",
    "\n",
    "    \n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_epoch, error = 0, 0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            out = model(images)\n",
    "            loss = F.cross_entropy(out, labels)        \n",
    "        loss_epoch += loss.item()\n",
    "        error += (out.argmax(1) != labels).sum().item()        \n",
    "    print('[Test] Loss: %.4f, Err: %d,' %(loss_epoch/i, error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dutta/2.7/lib/python2.7/site-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "# net = MLP().to(device)\n",
    "from models import resnet\n",
    "net = resnet.ResNet18().to(device)\n",
    "\n",
    "A_args = {'lr': 0.5, 'weight_decay': 0.0001}\n",
    "sgd_optim = optim.SGD(net.parameters(),**A_args)\n",
    "sgd_scheduler = optim.lr_scheduler.MultiStepLR(optimizer=sgd_optim, milestones=[100, 150])\n",
    "la_optim = LookAhead(net.parameters(), alpha=0.5, k=5, base_optim=sgd_optim, A_args=A_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Loss: 2.3178, Err: 9000,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-154dd2f97f32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mla_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-3d30673da176>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimiser, train_loader, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    test(net, test_loader, device)\n",
    "    train(net, la_optim, train_loader, device)\n",
    "    sgd_scheduler.step(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
